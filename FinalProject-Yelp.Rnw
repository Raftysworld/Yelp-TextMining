\documentclass[12pt]{article}

%AMS-TeX packages
\usepackage{amssymb,amsmath,amsthm} 
%geometry (sets margin) and other useful packages
\usepackage[margin=1in]{geometry}
\usepackage{graphicx, ctable, booktabs}
\usepackage{color}
\usepackage{listings}
\usepackage{ltablex, calc, enumerate, multirow, float, soul}
\usepackage{hyperref}
\usepackage{longtable, pdflscape, textcase}

\begin{document}
<<concordance, echo=FALSE>>=
opts_chunk$set(concordance=TRUE)
@

\setlength{\parskip}{3ex}
\setlength{\parindent}{0pt}

\title{STAT 503 Final Project}
\author{Jim Curro, Eric Hare, Alex Shum}
\date{Apr. 26, 2013}

\maketitle

<<LoadLibraries, echo=FALSE, cache=FALSE, message=FALSE>>=
    ## Libraries
    library(rjson)
    library(plyr)
    library(maps)
    library(tm)
    library(ggplot2)
    library(xtable)
    library(e1071)
    library(randomForest)
@

\section*{Introduction}
The dataset that we have includes reviews of the different businesses written by yelp users in the Phoenix, Arizona metropolitan area in the time frame of January 13 - March 12, 2013. Our analysis is using the reviews given to us and identify which reviews were classified as useful based on only the number of stars the user gave the business, the total number of reviews by the user, the average star given by the user, and the text data of the review. Using this data, we fit an SVM in order to predict the usefulness of the test set of reviews.

\section*{Data}
We began by taking the unparsed json files from Yelp and producing three main datasets: business.json, user.json, and reviews.json.

business.json has information for every business in the Phoenix area that yelp has recorded and includes the following variables (name of business, business id, address, type of business, \# of reviews, \# of checkins, \# of stars).  Because of the aim of our study the business dataset will be used as an initial data analysis to examine the differences between businesses and try to identify any potential concerns in the data.  Concerns we will be aware of include an employee from the business giving the business many “false” positive reviews of a business.  Also we can use this data to identify differences among businesses and analyze businesses that have a high percentage of reviews per check in.

user.json has all of the different information for each user that yelp keeps track of.  Within this dataset we have for each user the variables (name of user, user id, \# of funny reviews, \# of useful reviews, \# of cool reviews, \# average stars, total \# reviews).  Different from the business.json data there are some important variables in this dataset that is given in the test set that we can use to better our analysis of useful reviews.  By looking at the total number of reviews the user has and their average star count possibly we can identify a trend with these two variables and see how it pertains to predicting whether the users reviews are classified as useful or not.  There are also other variables given in this dataset that we can use to further explore our data that is given, however those will not be useful in our final analysis of predicting useful reviews.

reviews.json is our primary dataset for predicting whether a review is useful or not because it has every review recorded and all of the different information recorded by yelp for each review.  These variables include (review id, user id, business id, \# votes funny, \# votes useful, \# votes cool, \# stars given, date of review, text).  Based on the goals of our study we will only be concerned with whether the review was useful, the stars given by the user, and the text data to identify the unique aspects of a review that others find to be useful.

\section*{Initial Data Analysis}

<<DataProcessing, echo=FALSE, cache=TRUE>>=
    processData <- function(json) {
        lines <- readLines(json)
        json.lines <- lapply(1:length(lines), function(x) { fromJSON(lines[x])})
    }
    
    ## Read in json training files
    business.json <- processData("yelp_training_set_business.json")
    checkin.json <- processData("yelp_training_set_checkin.json")
    reviews.json <- processData("yelp_training_set_review.json")
    user.json <- processData("yelp_training_set_user.json")
    
    
    ## Reviews Data
    ## Convert to DF
    reviews.data <- data.frame(matrix(unlist(reviews.json), nrow = length(reviews.json), byrow = TRUE))
    names(reviews.data) <- c("funny", "useful", "cool", names(reviews.json[[1]])[-1])
    ## Fix some of the data types
    reviews.data$useful <- as.numeric(as.character(reviews.data$useful))
    reviews.data$cool <- as.numeric(as.character(reviews.data$cool))
    reviews.data$funny <- as.numeric(as.character(reviews.data$funny))
    
    
    ## Business Data
    ## We need to turn "Categories" into a comma separated string
    for (i in 1:length(business.json)) {
        business.json[[i]]$categories <- paste(business.json[[i]]$categories, collapse = ",")
    }
    business.data.tmp <- data.frame(matrix(unlist(business.json), nrow = length(business.json), byrow = TRUE))
    names(business.data.tmp) <- names(business.json[[1]])[-8]
    
    
    ## Checkin Data
    ##
    for (i in 1:length(checkin.json)) {
        checkins <- sum(unlist(checkin.json[[i]][-(c(length(checkin.json[[i]]), length(checkin.json[[i]]) - 1))]))
        business_id <- checkin.json[[i]]$business_id
        
        checkin.json[[i]] <- list(business_id = business_id, checkins = checkins)
    }
    checkin.data <- data.frame(matrix(unlist(checkin.json), nrow = length(checkin.json), byrow = TRUE))
    names(checkin.data) <- names(checkin.json[[1]])
    
    
    ## User Data
    user.data <- data.frame(matrix(unlist(user.json), nrow = length(user.json), byrow = TRUE))
    names(user.data) <- c("funny", "useful", "cool", names(user.json[[1]])[-1])
    user.data$useful <- as.numeric(as.character(user.data$useful))
    user.data$cool <- as.numeric(as.character(user.data$cool))
    user.data$funny <- as.numeric(as.character(user.data$funny))
    user.data$average_stars <- as.numeric(as.character(user.data$average_stars))
    user.data$review_count <- as.numeric(as.character(user.data$review_count))
    
    ####
    ## Merge Data
    ## Three sets, businesses and users, with reviews linking businesses to users
    ####
    business.data <- merge(business.data.tmp, checkin.data, by = "business_id")
    business.data$checkins <- as.numeric(as.character(business.data$checkins))
    business.data$review_count <- as.numeric(as.character(business.data$review_count))
    business.data$longitude <- as.numeric(as.character(business.data$longitude))
    business.data$stars <- as.numeric(as.character(business.data$stars))
    business.data$stars.f <- factor(business.data$stars)
    business.data$latitude <- as.numeric(as.character(business.data$latitude))
    
    reviews.data$text <- as.character(reviews.data$text)

    set.seed(20130421)
    data.sample <- sample(1:nrow(reviews.data), 20000)
    reviews.sub <- reviews.data[data.sample, ]

    rm(reviews.json)
    rm(users.json)
    rm(business.json)
    rm(checkin.json)
@

Figure \ref{fig:UsefulFunny} displays the number of useful votes against the number of cool votes in blue, and the number of useful votes against the number of funny votes in red.  It can be seen that while both cool and funny votes are great predictors of useful votes, the slope is larger for funny.  In other words, we would expect that the reviews with a set number of funny votes would tend to have more useful votes than those with the same set number of cool votes.  Note, however, that we ultimately chose not to use funny and cool as predictors because the Yelp rules disallowed this.

<<UsefulFunny, echo=FALSE, out.height='6in', out.width='6in', fig.show='hold', fig.align='center', fig.cap='Useful votes vs cool (blue) and funny(red) votes.  This plot indicates that a review which is voted as being funny and/or cool is also likely to be voted as being useful.', fig.pos='H'>>=
    qplot(cool, useful, data = user.data, colour = I("blue")) + 
        geom_point(aes(x = funny, y = useful), colour = I("red")) +
        xlab("Cool/Funny Votes") +
        ylab("Useful Votes")
@

Table \ref{tab:CityData} shows the top 10 cities by number of checkins in the database.  It also shows the total number of reviews for all businesses in that city, as well as the percentage of reviews over checkins.  It can be seen that for the biggest cities in the database, there is a very uniform number of about .27 reviews per checkin.

<<CityData, echo=FALSE, results='asis'>>=
    business.data$full_address <- as.character(business.data$full_address)
    
    city.data = ddply(business.data,.(city),summarize,
      reviews = sum(review_count),
      checkins = sum(checkins))
    
    city.data <- city.data[with(city.data, order(-checkins)), ]
    
    city.data$percentage <- city.data$reviews/city.data$checkins
    
    print(xtable(city.data[1:10,], label = "tab:CityData", caption = "Top ten cities by the number of checkins in that city"), include.rownames = FALSE, table.placement = 'H')
@

Figure \ref{fig:CheckinsBoxplot} shows boxplots of the number of checkins to each business by the average star rating of that business.  It can be observed that businesses with about a four star rating tend to have the most checkins, but five star rating businesses actually have fewer.  This may be either due to the fact that businesses with such a high rating have fewer total reviews, or they are more expensive and less likely to have a high number of customers.

<<CheckinsBoxplot, echo=FALSE, out.height='4.5in', out.width='4.5in', fig.show='hold', fig.align='center', fig.pos='H', fig.cap='Number of checkins to each business by average star rating of that business'>>=
    qplot(stars.f, log(checkins), data = business.data, geom = "boxplot", colour = stars.f) +
        xlab("Average star rating") +
        ylab("Number of checkins")
@

\section*{Exploratory Data Analysis}

Table \ref{tab:UsefulTable} displays the top ten users by the total number of useful votes divided by the total number of reviews for that user.  We have selected only users who have made at least 100 reviews.  We felt this would provide us with valuable information about what characteristics the ``best" reviewers possess.

<<UsefulTable, echo=FALSE, results='asis'>>=
    use = subset(user.data,review_count>100)
    use$good = use$useful / use$review
    use = use[with(use,order(-good)),]
    
    print(xtable(use[1:10, c(1:3, 5:7, 9)], caption = "Top ten users in the Yelp data by total number of useful votes per review (Minimum 100 reviews).", label = "tab:UsefulTable"), include.rownames = FALSE, table.placement = 'H')
@

Figure \ref{fig:NotAlexPlots} shows the number of useful reviews by the average number of stars for each user, colored by the user's total number of reviews.

<<NotAlexPlots, echo=FALSE, fig.cap='Displays for each user their number of useful reviews by the average stars that each user gives any review. The users are colored by the total number of reviews showing a clear trend in number of reviews and number of useful reviews', fig.pos='H', fig.show='hold', fig.align='center'>>=
    user.data$frc = '<50'
    user.data$frc[user.data$review_count > 50] = '50-200'
    user.data$frc[user.data$review_count > 200] = '200-500'
    user.data$frc[user.data$review_count > 500] = '500-1000'
    user.data$frc[user.data$review_count > 1000] = '>1000'

    qplot(data=user.data,average_stars,useful,color = frc) +
        xlab("Average star rating") +
        ylab("Number of useful reviews")
@

The results of this plot are very encouraging that the average stars a user gives will be a strong predictor of useful reviews. The plot demonstrates that users with average stars given between 3 and 4.5 are most likely to have reviews that are the most useful. It also shows that users with more total reviews are highly likely to have more useful reviews.  

\section*{Preprocessing and Classification}
The original collection of reviews was provided by Yelp with minimal preprocessing. Starting
with the text reviews written by yelp users (our corpus), we removed whitespace, put words
into lowercase, removed all punctuation, removed stop words, and stemmed words to put
them into the same form. A standard text model is the “bag of word” model in which a corpus
is represented as a term-document matrix; each word that appears in the corpus is a column
and individual reviews (documents) are rows. From our original corpus we only included words
that appear in at least 10 reviews. The resulting term document matrix is a 229,907 by 27,876
sparse matrix.

Our response variable is the number of ‘useful’ votes given to a particular review. In order to
turn this into a classification problem we categorized reviews as useful if at least one person
voted it useful. Roughly half the data has zero useful votes and the rest of the reviews have
more than one useful vote. <add in description of response variable>

\subsection*{R Memory and Computation Time Issues}
Building a term document matrix, reducing dimension using singular value decomposition and classifying based on the rows or columns of the matrix is a fairly standard method in text mining. Unfortunately we ran into a number of memory issues in R.

The tm package in R by default outputs sparse matrices. Many of the classification algorithms as implemented in R do not support sparse matrices. It was also not possible to convert sparse matrices into regular matrix objects in R due to the size and dimensionality of our dataset.  The SVM function in e1071 does support sparse matrices but for a large dataset training time is a major issue.  We tried training an SVM and we were not able to fit the model even after a full day of computer time.

An alternative approach was to further reduce the number of dimensions. After removing stop words and running the corpus through a stemming algorithm we removed overly sparse words. Words that appeared in fewer than 10 documents were removed. We were able to reduce the number of columns from 27,826 to around 5,000. However the resulting matrix was still too large to fit in memory as a normal matrix object. We were able to reduce the dimensions further by sampling 20,000 documents from the original 229,907. The resulting matrix could fit into memory as a normal matrix object but even then we ran into memory and time issues with both SVM and random forests.

We considered sampling from an even smaller subset of the dataset but we were already reluctant to use a dataset of this size due to the dimensionality and sparsity. If memory constraints were not an issue we would need to construct a term document matrix not only with the training data but also the test data. Unfortunately it seems we’ve reached the limits of base R.

\section*{Classification}

Since the text mining approach is not feasible in R we classified based on some summary
statistics of the text data. We computed summary statistics on an 18,000 review subset of
the original unprocessed text of the reviews and then tabled the results by whether or not the
reviews were voted as useful (seen in Table \ref{tab:SummaryTable}).

<<SummaryTable, echo=FALSE, results='asis', cache=TRUE>>=
    rm(reviews.data)

    reviews.sub$numChar <- sapply(reviews.sub$text, nchar)
    reviews.sub$numCap <- sapply(reviews.sub$text, function(x) { length(grep("[A-Z]", strsplit(as.character(x), split = "")[[1]])) / (nchar(x) + 1)})
    reviews.sub$numPunc <- sapply(reviews.sub$text, function(x) { length(grep("[^a-zA-Z ]", strsplit(as.character(x), split = "")[[1]])) / (nchar(x) + 1)})
    reviews.sub$numPar <- sapply(reviews.sub$text, function(x) { length(grep("\n", strsplit(as.character(x), split = "")[[1]])) / (nchar(x) + 1)})


    important.words <- read.csv("m.csv")
    reviews.sub$hasDont <- sapply(reviews.sub$text, function(x) { length(grep(" don'*t ", tolower(as.character(x)))) > 0})
    reviews.sub$hasTime <- sapply(reviews.sub$text, function(x) { length(grep(" time[sd]* ", tolower(as.character(x)))) > 0})

    reviews.final <- merge(reviews.sub, user.data[,c("user_id", "average_stars", "cool", "funny", "useful")], by = "user_id", all.x = TRUE, suffixes = c(".review", ".user"))
    
    reviews.final$useful_bin <- reviews.final$useful.review > 0
    summary.sub <- ddply(reviews.final, .(useful_bin), summarise, numChar = mean(numChar), numCap = mean(numCap), numPunc = mean(numPunc), numPar = mean(numPar), hasDont = mean(hasDont), hasTime = mean(hasTime))

    print(xtable(summary.sub, label = "tab:SummaryTable", caption = "Summary statistics for all reviews by whether the review was voted as useful or not.  The variables include the number of characters, percentage of letters capitalized, percentage of the review containing punctuation, the percentage of reviews that contain the word dont, and the percentage of reviews that contain the word time"), table.placement = 'H')
@

The variables we considered were number of stars a review gave, number of characters
in a review, number of capital letters used, number of punctuation marks used, number of
paragraphs and whether or not the review contains the words ``time" and ``don’t".
We reasoned that longer reviews contained more useful information and useful reviews are
more likely to contain proper punctuation, proper capitalization and more likely to be structured
in paragraphs. We also calculated the word frequencies for useful reviews versus the word
frequencies for unuseful reviews. Based on word frequencies we calculated the probabilities
of words that are likely to appear in useful versus unuseful reviews. We found that the words
“don’t” and “time” are much more likely to appear in a useful review than an unuseful review.
Additionally, we reasoned that a review with a higher star rating would be more likely to have a
useful explanation to justify the star rating.

We initially fit all of the variables into a random forest classifier and used mean decrease in
accuracy and mean decrease gini from the random forest algorithm to find the most important
variables. The results are displayed in Table \ref{tab:RandomForest}.

<<ModelDef, echo=FALSE, cache=TRUE>>=
    classError <- function(table) {
        cls1 <- (table[1,2] / (table[1,2] + table[1,1]))
        cls2 <- (table[2,1] / (table[2,2] + table[2,1]))
        #cat(paste("Not Useful:", cls1, "\n"))
        #cat(paste("Useful:", cls2, "\n"))
        #cat(paste("Total:", mean(c(cls1, cls2)), "\n"))
        
        return(mean(c(cls1, cls2)))
    }

    reviews.train <- reviews.final[1:18000, ]
    reviews.test <- reviews.final[18001:20000, -c(1:3, ncol(reviews.final))]
    reviews.test.truth <- reviews.final[18001:20000, ncol(reviews.final)]

    reviews.rf <- randomForest(factor(useful_bin) ~ stars+numChar+numCap+numPunc+numPar+hasDont+hasTime, data = reviews.train, importance = TRUE, ntree = 1000)

    reviews.svm <- svm(factor(useful_bin) ~ numChar+numPar+numPunc, data = reviews.train, kernel = "linear")

    predict.svm <- predict(reviews.svm, reviews.test)
    #classError(table(predict.svm, reviews.test.truth))
@


<<RandomForest, echo=FALSE, results='asis'>>=
    print(xtable(reviews.rf$importance, label = "tab:RandomForest", caption = "A list of the variables and their importance as determined by the randomForest algorithm.  We ultimately selected three of these variables, numChar, numCap, and numPunc for use in our SVM."), table.placement = 'H')
@

From the previous importance results, we fit a model on an 18,000 document subset of our
original dataset to predict usefulness based on the number of characters, capitalization and
number of paragraphs. We tested our model on a separate randomly selected 2,000 document
subset of our original dataset. Our overall accuracy is \Sexpr{classError(table(predict.svm, reviews.test.truth))}\%. The full classification results can be seen in the confusion matrix displayed in Table \ref{tab:SVM}.

<<SVM, echo=FALSE, results='asis'>>=
    tbl <- table(predict.svm, reviews.test.truth)
    colnames(tbl) <- c("True Useful", "True Not Useful")
    rownames(tbl) <- c("Predicted Useful", "Predicted Not Useful")

    print(xtable(table(predict.svm, reviews.test.truth), label = "tab:SVM", caption = "Truth Table for the results of our model"), table.placement = 'H')
@

\section*{Conclusion}
From our initial data analysis of the Yelp dataset we found that useful votes are correlated with both cool and funny votes, most businesses have an average star rating of four stars and, unsurprisingly, Yelp users who wrote the most reviews also had the most useful votes.  The biggest challenge to this project was computational in nature.  We were unable to use the text data in its entirety to predict review usefulness.  Using summary statistics on the text data we were able to achieve around 62 percent accuracy using an SVM.  We can conclude from our results that review usefulness can be  partially predicted solely on the text structure rather than content.  Useful reviews tended to be longer, structured in paragraphs and with punctuation.  With additional resources we would expand our analysis to a model containing structure, text content, user information and business information.  We feel we can drastically improve our results by including text data and background information on users and businesses (number of reviews, average rating of reviews).

% APPENDIX : Role each group member played

\end{document}