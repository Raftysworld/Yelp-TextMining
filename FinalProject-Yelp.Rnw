\documentclass[11pt]{article}

%AMS-TeX packages
\usepackage{amssymb,amsmath,amsthm} 
%geometry (sets margin) and other useful packages
\usepackage[margin=1in]{geometry}
\usepackage{graphicx, ctable, booktabs}
\usepackage{color}
\usepackage{listings}
\usepackage{ltablex, calc, enumerate, multirow, float, soul}
\usepackage{hyperref}
\usepackage{longtable, pdflscape, textcase}

\begin{document}
<<concordance, echo=FALSE>>=
opts_chunk$set(concordance=TRUE)
@

\setlength{\parskip}{1ex}
\setlength{\parindent}{0pt}

\title{STAT 503 Final Project}
\author{Jim Curro, Eric Hare, Alex Shum}
\date{Apr. 26, 2013}

\maketitle

<<LoadLibraries, echo=FALSE, cache=FALSE, message=FALSE>>=
    ## Libraries
    library(rjson)
    library(plyr)
    library(maps)
    library(tm)
    library(ggplot2)
    library(xtable)
    library(e1071)
    library(randomForest)
@

\section*{Introduction}
The dataset that we have includes reviews of the different businesses written by yelp users in the Phoenix, Arizona metropolitan area in the time frame of January 13 - March 12, 2013. Our analysis is using the reviews given to us and identify which reviews were classified as useful based on only the number of stars the user gave the business, the total number of reviews by the user, the average star given by the user, and the text data of the review. Using this data, we fit an SVM in order to predict the usefulness of the test set of reviews.

\section*{Data}
We began by taking the unparsed json files from Yelp and producing three main datasets: business.json, user.json, and reviews.json.

business.json has information for every business in the Phoenix area that yelp has recorded and includes the following variables (name of business, business id, address, type of business, \# of reviews, \# of checkins, \# of stars).  Because of the aim of our study the business dataset will be used as an initial data analysis to examine the differences between businesses and try to identify any potential concerns in the data.  Concerns we will be aware of include an employee from the business giving the business many “false” positive reviews of a business.  Also we can use this data to identify differences among businesses and analyze businesses that have a high percentage of reviews per check in.

user.json has all of the different information for each user that yelp keeps track of.  Within this dataset we have for each user the variables (name of user, user id, \# of funny reviews, \# of useful reviews, \# of cool reviews, \# average stars, total \# reviews).  Different from the business.json data there are some important variables in this dataset that is given in the test set that we can use to better our analysis of useful reviews.  By looking at the total number of reviews the user has and their average star count possibly we can identify a trend with these two variables and see how it pertains to predicting whether the users reviews are classified as useful or not.  There are also other variables given in this dataset that we can use to further explore our data that is given, however those will not be useful in our final analysis of predicting useful reviews.

reviews.json is our primary dataset for predicting whether a review is useful or not because it has every review recorded and all of the different information recorded by yelp for each review.  These variables include (review id, user id, business id, \# votes funny, \# votes useful, \# votes cool, \# stars given, date of review, text).  Based on the goals of our study we will only be concerned with whether the review was useful, the stars given by the user, and the text data to identify the unique aspects of a review that others find to be useful.

\section*{Initial Data Analysis}

<<DataProcessing, echo=FALSE, cache=TRUE>>=
    processData <- function(json) {
        lines <- readLines(json)
        json.lines <- lapply(1:length(lines), function(x) { fromJSON(lines[x])})
    }
    
    ## Read in json training files
    business.json <- processData("yelp_training_set_business.json")
    checkin.json <- processData("yelp_training_set_checkin.json")
    reviews.json <- processData("yelp_training_set_review.json")
    user.json <- processData("yelp_training_set_user.json")
    
    
    ## Reviews Data
    ## Convert to DF
    reviews.data <- data.frame(matrix(unlist(reviews.json), nrow = length(reviews.json), byrow = TRUE))
    names(reviews.data) <- c("funny", "useful", "cool", names(reviews.json[[1]])[-1])
    ## Fix some of the data types
    reviews.data$useful <- as.numeric(as.character(reviews.data$useful))
    reviews.data$cool <- as.numeric(as.character(reviews.data$cool))
    reviews.data$funny <- as.numeric(as.character(reviews.data$funny))
    
    
    ## Business Data
    ## We need to turn "Categories" into a comma separated string
    for (i in 1:length(business.json)) {
        business.json[[i]]$categories <- paste(business.json[[i]]$categories, collapse = ",")
    }
    business.data.tmp <- data.frame(matrix(unlist(business.json), nrow = length(business.json), byrow = TRUE))
    names(business.data.tmp) <- names(business.json[[1]])[-8]
    
    
    ## Checkin Data
    ##
    for (i in 1:length(checkin.json)) {
        checkins <- sum(unlist(checkin.json[[i]][-(c(length(checkin.json[[i]]), length(checkin.json[[i]]) - 1))]))
        business_id <- checkin.json[[i]]$business_id
        
        checkin.json[[i]] <- list(business_id = business_id, checkins = checkins)
    }
    checkin.data <- data.frame(matrix(unlist(checkin.json), nrow = length(checkin.json), byrow = TRUE))
    names(checkin.data) <- names(checkin.json[[1]])
    
    
    ## User Data
    user.data <- data.frame(matrix(unlist(user.json), nrow = length(user.json), byrow = TRUE))
    names(user.data) <- c("funny", "useful", "cool", names(user.json[[1]])[-1])
    user.data$useful <- as.numeric(as.character(user.data$useful))
    user.data$cool <- as.numeric(as.character(user.data$cool))
    user.data$funny <- as.numeric(as.character(user.data$funny))
    user.data$average_stars <- as.numeric(as.character(user.data$average_stars))
    user.data$review_count <- as.numeric(as.character(user.data$review_count))
    
    ####
    ## Merge Data
    ## Three sets, businesses and users, with reviews linking businesses to users
    ####
    business.data <- merge(business.data.tmp, checkin.data, by = "business_id")
    business.data$checkins <- as.numeric(as.character(business.data$checkins))
    business.data$review_count <- as.numeric(as.character(business.data$review_count))
    business.data$longitude <- as.numeric(as.character(business.data$longitude))
    business.data$stars <- as.numeric(as.character(business.data$stars))
    business.data$stars.f <- factor(business.data$stars)
    business.data$latitude <- as.numeric(as.character(business.data$latitude))
    
    reviews.data$text <- as.character(reviews.data$text)

    set.seed(20130421)
    data.sample <- sample(1:nrow(reviews.data), 20000)
    reviews.sub <- reviews.data[data.sample, ]
@

Figure \ref{fig:UsefulFunny} displays the number of useful votes against the number of cool votes in blue, and the number of useful votes against the number of funny votes in red.  It can be seen that while both cool and funny votes are great predictors of useful votes, the slope is larger for funny.  In other words, we would expect that the reviews with a set number of funny votes would tend to have more useful votes than those with the same set number of cool votes.  Note, however, that we ultimately chose not to use funny and cool as predictors because the Yelp rules disallowed this.

<<UsefulFunny, echo=FALSE, out.height='5in', out.width='5in', fig.show='hold', fig.align='center', fig.cap='Useful votes vs cool (blue) and funny(red) votes.  This plot indicates that a review which is voted as being funny and/or cool is also likely to be voted as being useful.', fig.pos='H'>>=
qplot(cool, useful, data = user.data, colour = I("blue")) + 
    geom_point(aes(x = funny, y = useful), colour = I("red")) +
    xlab("Cool/Funny Votes") +
    ylab("Useful Votes")
@

Table \ref{tab:CityData} shows the top 10 cities by number of checkins in the database.  It also shows the total number of reviews for all businesses in that city, as well as the percentage of reviews over checkins.  It can be seen that for the biggest cities in the database, there is a very uniform number of about .27 reviews per checkin.

<<CityData, echo=FALSE, results='asis'>>=
business.data$full_address <- as.character(business.data$full_address)

business.data$zip <- substring(business.data$full_address, nchar(business.data$full_address) - 5, nchar(business.data$full_address))

city.data = ddply(business.data,.(city),summarize,
  reviews = sum(review_count),
  checkins = sum(checkins))

city.data <- city.data[with(city.data, order(-checkins)), ]

city.data$percentage <- city.data$reviews/city.data$checkins

print(xtable(city.data[1:10,], label = "tab:CityData", caption = "Top ten cities by the number of checkins in that city"), include.rownames = FALSE, table.placement = '!h')
@

Figure \ref{fig:CheckinsBoxplot} shows boxplots of the number of checkins to each business by the average star rating of that business.  It can be observed that businesses with about a four star rating tend to have the most checkins, but five star rating businesses actually have fewer.  This may be either due to the fact that businesses with such a high rating have fewer total reviews, or they are more expensive and less likely to have a high number of customers.

<<CheckinsBoxplot, echo=FALSE, out.height='5in', out.width='5in', fig.show='hold', fig.align='center', fig.pos='H', fig.cap='Number of checkins to each business by average star rating of that business'>>=
qplot(stars.f, log(checkins), data = business.data, geom = "boxplot", colour = stars.f)
@

Table \ref{tab:UsefulTable} displays the top ten users by the total number of useful votes divided by the total number of reviews for that user.  We have selected only users who have made at least 100 reviews.  We felt this would provide us with valuable information about what characteristics the ``best" reviewers possess.

<<UsefulTable, echo=FALSE, results='asis'>>=
use = subset(user.data,review_count>100)
use$good = use$useful / use$review
use = use[with(use,order(-good)),]

print(xtable(use[1:10, c(1:3, 5:7, 9)], caption = "Top ten users in the Yelp data by total number of useful votes per review (Minimum 100 reviews).", label = "tab:UsefulTable"), include.rownames = FALSE, table.placement = '!h')
@

<<NotAlexPlots, echo=FALSE, fig.cap='Displays for each user their number of useful reviews by the average stars that each user gives any review. The users are colored by the total number of reviews showing a clear trend in number of reviews and number of useful reviews'>>=
    user.data$frc = '<50'
    user.data$frc[user.data$review_count > 50] = '50-200'
    user.data$frc[user.data$review_count > 200] = '200-500'
    user.data$frc[user.data$review_count > 500] = '500-1000'
    user.data$frc[user.data$review_count > 1000] = '>1000'
    qplot(data=user.data,average_stars,useful,color = frc)
@

The results of this plot are very encouraging that the average stars a user gives will be a strong predictor of useful reviews. Figure XX displays that users with average stars given between 3 and 4.5 are most likely to have reviews that are the most useful. It also shows that users with more total reviews are highly likely to have more useful reviews.  

\section*{Preprocessing and Classification}
The review data provided by Yelp was provided without much preprocessing.  Starting with the corpus we removed excess whitespace, put all words into lower case, removed all punctuation, removed numbers and removed stop words (stop words are common words that do not provide much meaning such as 'the', 'is' and 'or').  Our final step of preprocessing is to stem the corpus (reducing words to their base form).  Unfortunately, the stemming algorithm used in the TM package has some external dependencies on Weka and Java.  For our term document matrix we only included words that appear in at least 10 reviews.  The resulting matrix is a 27826 by 229907 sparse matrix.  The first 10 x 10 block of the matrix can be seen below in Table X.

\subsection*{R Memory Issues}
Building a term document matrix, reducing dimension using singular value decomposition and classifying based on the rows or columns of the matrix is a fairly standard method in text mining. Unfortunately we ran into a number of memory issues in R.

The tm package in R by default outputs sparse matrices. Many of the classification algorithms as implemented in R do not support sparse matrices. It was also not possible to convert sparse matrices into regular matrix objects in R due to the size and dimensionality of our dataset.

An alternative approach was to further reduce the number of dimensions. After removing stop words and running the corpus through a stemming algorithm we removed overly sparse words. Words that appeared in fewer than 10 documents were removed. We were able to reduce the number of columns from 27,826 to around 5,000. However the resulting matrix was still too large to fit in memory as a normal matrix object. We were able to reduce the dimensions further by sampling 20,000 documents from the original 229,907. The resulting matrix could fit into memory but this subset of our data was still too large to train an SVM or random forest.

We considered sampling from an even smaller subset of the dataset but we were already reluctant to use a dataset of this size due to the dimensionality and sparsity. If memory constraints were not an issue we would need to construct a term document matrix not only with the training data but also the test data. Unfortunately it seems we’ve reached the limits of base R.

We computed summary statistics on the text of the reviews and then tabled the results by whether or not the reviews were voted as being useful.  This helped us to see some factors of reviews that were most correlated with being useful.  They are displayed in Table X.

<<SummaryTable, echo=FALSE, results='asis', cache=TRUE>>=
    reviews.sub$numChar <- sapply(reviews.sub$text, nchar)
    reviews.sub$numCap <- sapply(reviews.sub$text, function(x) { length(grep("[A-Z]", strsplit(as.character(x), split = "")[[1]])) / (nchar(x) + 1)})
    reviews.sub$numPunc <- sapply(reviews.sub$text, function(x) { length(grep("[^a-zA-Z ]", strsplit(as.character(x), split = "")[[1]])) / (nchar(x) + 1)})
    reviews.sub$numPar <- sapply(reviews.sub$text, function(x) { length(grep("\n", strsplit(as.character(x), split = "")[[1]])) / (nchar(x) + 1)})


    important.words <- read.csv("m.csv")
    reviews.sub$hasDont <- sapply(reviews.sub$text, function(x) { length(grep(" don'*t ", tolower(as.character(x)))) > 0})
    reviews.sub$hasTime <- sapply(reviews.sub$text, function(x) { length(grep(" time[sd]* ", tolower(as.character(x)))) > 0})
    
    reviews.sub$useful_bin <- reviews.sub$useful > 0
    summary.sub <- ddply(reviews.sub, .(useful_bin), summarise, numChar = mean(numChar), numCap = 100 * mean(numCap), numPunc = 100 * mean(numPunc), numPar = 100 * mean(numPar), hasDont = mean(hasDont), hasTime = mean(hasTime))

    print(xtable(summary.sub), table.placement = '!h')
@

blah...

<<RandomForest, echo=FALSE, results='asis', cache=TRUE>>=
    classError <- function(table) {
        cls1 <- (table[1,2] / (table[1,2] + table[1,1]))
        cls2 <- (table[2,1] / (table[2,2] + table[2,1]))
        cat(paste("Not Useful:", cls1, "\n"))
        cat(paste("Useful:", cls2, "\n"))
        cat(paste("Total:", mean(c(cls1, cls2)), "\n"))
    }

    reviews.train <- reviews.sub[1:18000, ]
    reviews.test <- reviews.sub[18001:20000, -c(1:3, ncol(reviews.sub))]
    reviews.test.truth <- reviews.sub[18001:20000, ncol(reviews.sub)]

    reviews.rf <- randomForest(factor(useful_bin) ~ stars+numChar+numCap+numPunc+numPar+hasDont+hasTime, data = reviews.train, importance = TRUE, ntree = 1000)

    print(xtable(reviews.rf$importance), table.placement = '!h')
@

Blah blah blah

<<SVM, echo=FALSE, results='asis'>>=
    reviews.svm <- svm(factor(useful_bin) ~ numChar+numPar+numPunc, data = reviews.train, kernel = "linear")

    predict.svm <- predict(reviews.svm, reviews.test)
    #classError(table(predict.svm, reviews.test.truth))

    print(xtable(table(predict.svm, reviews.test.truth)), table.placement = '!h')
@


\end{document}